# -*- coding: utf-8 -*-
"""
Pipeline unificado com LOGS: Transforma√ß√µes (original, log+Œµ, bootstrap FPP) + Modelos
(Croston, SBA, TSB, RandomForest, SARIMAX, LSTM opcional) com registro de experimentos.

Crit√©rio do campe√£o (FPP3 pgs anexadas): minimizar MAE (escala original). Desempates por RMSE.

Sa√≠das:
  - .../experimentos_unificado.xlsx         -> aba "experiments" + aba "champion"
  - .../experimentos_unificado.csv
  - .../champion.csv

üîå Streamlit hint:
- os prints via `log()` podem ser encaminhados para `st.status()` / `st.write()`
- o DataFrame final pode ser exibido com `st.dataframe()` e disponibilizado para download
"""

import os, time, warnings, itertools, sys, datetime as dt
from dataclasses import dataclass
from typing import Tuple, Dict, List, Optional, Callable, Union

import numpy as np
import pandas as pd

from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import MinMaxScaler

from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.seasonal import STL
from scipy.stats import boxcox, boxcox_normmax

warnings.filterwarnings("ignore")

# ============================
# LOGGING UTIL
# ============================
def _now():
    """Retorna hor√°rio hh:mm:ss para prefixar logs."""
    return dt.datetime.now().strftime("%H:%M:%S")

def log(msg: str):
    """
    Logger simples que imprime com timestamp.

    üîå Streamlit hint:
    - troque por `st.write(msg)` ou acumule mensagens num buffer e mostre no app.
    """
    print(f"[{_now()}] {msg}", flush=True)

class Timer:
    """
    Context manager para medir tempo de blocos de c√≥digo.
    Exemplo:
        with Timer("Treinando SARIMA"):
            ... c√≥digo ...
    """
    def __init__(self, label: str):
        self.label = label
        self.t0 = None
    def __enter__(self):
        self.t0 = time.time()
        log(f"‚ñ∂ {self.label} ‚Äî in√≠cio")
        return self
    def __exit__(self, exc_type, exc, tb):
        dt_s = time.time() - self.t0
        log(f"‚ñ† {self.label} ‚Äî fim em {dt_s:.2f}s")

# ============================
# CONFIGS
# ============================
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# Grades de hiperpar√¢metros dos modelos cl√°ssicos e ML
CROSTON_ALPHAS = [0.05, 0.1, 0.2, 0.3, 0.5]
SBA_ALPHAS     = [0.05, 0.1, 0.2, 0.3, 0.5]
TSB_ALPHA_GRID = [0.1, 0.3, 0.5]
TSB_BETA_GRID  = [0.1, 0.3, 0.5]

RF_LAGS_GRID = [3, 6, 12]                 # cria lags 1..k
RF_N_ESTIMATORS_GRID = [200, 500]
RF_MAX_DEPTH_GRID = [None, 5, 10]

# grade compacta para SARIMA; pode ser aberta em produ√ß√£o
SARIMA_GRID = {"p":[0,1,2], "d":[0,1], "q":[0,1,2], "P":[0,1], "D":[0,1], "Q":[0,1]}

# LSTM opcional: o c√≥digo apenas roda se TensorFlow estiver presente
try:
    import tensorflow as tf  # type: ignore
    from tensorflow.keras.models import Sequential  # type: ignore
    from tensorflow.keras.layers import LSTM, Dense  # type: ignore
    KERAS_AVAILABLE = True
except Exception:
    KERAS_AVAILABLE = False

# ============================
# M√âTRICAS (calculadas SEMPRE na escala original quando h√° invers√£o)
# ============================
def eval_metrics(y_true, y_pred) -> Dict[str, float]:
    """
    Retorna MAE, MAPE, RMSE, sMAPE em dicion√°rio.
    - MAPE segura para zeros (retorna NaN se n√£o houver valores != 0).
    - sMAPE no formato 2|≈∑ - y|/(|y|+|≈∑|); inclu√≠da apenas para refer√™ncia.
    """
    y_true = np.asarray(y_true, dtype=float).ravel()
    y_pred = np.asarray(y_pred, dtype=float).ravel()
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.sum() == 0:
        return {"MAE": np.nan, "MAPE": np.nan, "RMSE": np.nan, "sMAPE": np.nan}
    y_true = y_true[mask]; y_pred = y_pred[mask]
    def _mape(a, f):
        m = a != 0
        return np.nan if m.sum() == 0 else 100 * np.mean(np.abs((a[m] - f[m]) / a[m]))
    mae  = float(np.mean(np.abs(y_true - y_pred)))
    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))
    smap = float(100 * np.mean(2*np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-9)))
    mape = float(_mape(y_true, y_pred))
    return {"MAE": mae, "MAPE": mape, "RMSE": rmse, "sMAPE": smap}

# ============================
# CARREGAMENTO E PADRONIZA√á√ÉO DA S√âRIE
# ============================
def _load_series_from_excel(file_path: str, sheet_name=None, date_col=None, value_col=None) -> pd.Series:
    """
    L√™ arquivo Excel, tenta inferir colunas de data/valor, agrega por m√™s e
    retorna S√©rie mensal cont√≠nua (freq='MS'), preenchida por interpola√ß√£o.
    """
    log(f"Lendo Excel: {file_path}")
    if sheet_name is None:
        df = pd.read_excel(file_path, sheet_name=0, engine="openpyxl")
    else:
        df = pd.read_excel(file_path, sheet_name=sheet_name, engine="openpyxl")

    # Se vier um dict (m√∫ltiplas abas), escolha a primeira n√£o vazia
    if isinstance(df, dict):
        for _, sub in df.items():
            if isinstance(sub, pd.DataFrame) and not sub.empty:
                df = sub; break
    df = df.dropna(axis=1, how='all')

    # Infer√™ncia leve dos nomes de colunas
    if date_col is None or value_col is None:
        cand_date  = {"date","data","mes","m√™s","month","ds"}
        cand_value = {"valor","volume","qtd","quantidade","demand","demanda","y","value"}
        low = {c: str(c).strip().lower() for c in df.columns}
        if date_col is None:
            for c in df.columns:
                if low[c] in cand_date: date_col = c; break
        if value_col is None:
            for c in df.columns:
                if low[c] in cand_value: value_col = c; break
        if date_col is None or value_col is None:
            usable = [c for c in df.columns if df[c].notna().sum() > 0]
            if len(usable) < 2:
                raise ValueError("Planilha precisa ter ao menos 2 colunas (data e valor).")
            date_col  = date_col  or usable[0]
            value_col = value_col or usable[1]

    log(f"Colunas detectadas: data='{date_col}', valor='{value_col}'")

    # Convers√µes e limpeza
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
    df = df.dropna(subset=[date_col])
    vals = pd.to_numeric(df[value_col], errors="coerce")
    # fallback para strings com pontua√ß√£o BR
    if vals.isna().mean() > 0.2:
        vals = (df[value_col].astype(str).str.replace(".", "", regex=False).str.replace(",", ".", regex=False))
        vals = pd.to_numeric(vals, errors="coerce")
    df[value_col] = vals
    df = df.dropna(subset=[value_col]).sort_values(date_col)

    # Agrega por m√™s e cria √≠ndice MS cont√≠nuo
    df["_M"] = df[date_col].dt.to_period("M")
    s = df.groupby("_M")[value_col].sum()
    s.index = s.index.to_timestamp(how="start")
    s = s.asfreq("MS").interpolate("linear").bfill().ffill().astype(float)
    s.name = value_col
    log(f"S√©rie mensal carregada: {len(s)} pontos, de {s.index.min().date()} a {s.index.max().date()}")
    return s

def ensure_monthly_series(df: pd.DataFrame, date_col: str = "ds", value_col: str = "y") -> pd.Series:
    """
    Converte um DataFrame com colunas (ds, y) para S√©rie mensal (freq='MS'),
    agregando por m√™s e preenchendo lacunas simples.
    """
    s = df.copy()
    s[date_col] = pd.to_datetime(s[date_col], errors="coerce", dayfirst=True)
    s = s[[date_col, value_col]].dropna(subset=[date_col])
    s["__m"] = s[date_col].dt.to_period("M").dt.to_timestamp(how="start")
    s = s.groupby("__m", as_index=True)[value_col].sum().sort_index()
    full_idx = pd.date_range(s.index.min(), s.index.max(), freq="MS")
    s = s.reindex(full_idx).fillna(0.0).astype(float)
    s.name = value_col
    log(f"S√©rie mensal padronizada: {len(s)} pontos")
    return s

def load_series(data_input: Union[str, pd.DataFrame, pd.Series], sheet_name=None, date_col=None, value_col=None) -> pd.Series:
    """
    Entrada flex√≠vel:
      - caminho Excel
      - DataFrame com ('ds','y') ou especificando `date_col`/`value_col`
      - Series com √≠ndice DatetimeIndex
    """
    if isinstance(data_input, pd.Series):
        s = data_input.copy()
        if s.index.freqstr != "MS": s = s.asfreq("MS")
        s = s.interpolate("linear").bfill().ffill().astype(float)
        log(f"Entrada: Series ({len(s)} pontos)")
        return s
    elif isinstance(data_input, pd.DataFrame):
        log("Entrada: DataFrame")
        if set(["ds","y"]).issubset(set(c.lower() for c in data_input.columns)):
            cols = {c.lower(): c for c in data_input.columns}
            df = data_input.rename(columns={cols["ds"]:"ds", cols["y"]:"y"})
        else:
            df = data_input.copy()
            if date_col is None or value_col is None:
                raise ValueError("Informe 'ds'/'y' ou date_col/value_col.")
        return ensure_monthly_series(df if 'ds' in df.columns else data_input, date_col=date_col or "ds", value_col=value_col or "y")
    else:
        return _load_series_from_excel(str(data_input), sheet_name, date_col, value_col)

# ============================
# TRANSFORMA√á√ÉO LOG + Œµ (Œµ escolhido para reduzir correla√ß√£o n√≠vel-vari√¢ncia)
# ============================
def correlacao_media_desvio(series: pd.Series, window: int = 6) -> float:
    """
    Correla√ß√£o entre m√©dia m√≥vel e desvio-padr√£o m√≥vel (proxy de heterocedasticidade).
    Quanto mais pr√≥ximo de zero ap√≥s a transforma√ß√£o, melhor a estabiliza√ß√£o de vari√¢ncia.
    """
    m = series.rolling(window).mean(); s = series.rolling(window).std(ddof=0)
    valid = m.notna() & s.notna()
    return np.nan if valid.sum() < 3 else float(m[valid].corr(s[valid]))

def escolher_epsilon(y: pd.Series, window: int = 6) -> Tuple[float, float, float]:
    """
    Varre uma grade de Œµ proporcional ao menor positivo de y (ap√≥s shift)
    e escolhe o que minimiza |corr(m√©dia m√≥vel, desvio m√≥vel)| no log.
    Retorna: (epsilon, score, shift aplicado)
    """
    shift = 0.0
    if (y <= 0).any(): shift = float(1 - y.min())  # garante positividade
    y_shift = y + shift
    min_pos = y_shift[y_shift > 0].min()
    base = np.array([0, 1e-6, 0.01, 0.05, 0.1, 0.5, 1.0])
    candidatos = np.unique(base * float(min_pos))
    melhor_eps, melhor_score = None, np.inf
    for eps in candidatos:
        y_log = np.log(y_shift + eps)
        score = abs(correlacao_media_desvio(y_log, window))
        if score < melhor_score: melhor_eps, melhor_score = float(eps), float(score)
    return float(melhor_eps), float(melhor_score), float(shift)

def make_log_transformers(s: pd.Series, window: int = 6):
    """
    Constr√≥i fun√ß√µes forward/inverse para log(y+shift+Œµ) e seu inverso.
    Retorna tamb√©m um texto com par√¢metros para logging/tabela.
    """
    eps, score, shift = escolher_epsilon(s, window)
    log(f"Transforma√ß√£o LOG: epsilon={eps:.6g}, shift={shift:.6g}, score={score:.4g}")
    def fwd(x: pd.Series) -> pd.Series: return np.log(x.astype(float) + shift + eps)
    def inv(arr: np.ndarray) -> np.ndarray: return np.exp(np.asarray(arr, dtype=float)) - shift - eps
    params_txt = f"epsilon={eps:.6g}, shift={shift:.6g}, score={score:.4g}"
    return fwd, inv, params_txt

# ============================
# BOX‚ÄìCOX + STL + BOOTSTRAP (FPP-style)
# ============================
@dataclass
class BoxCoxParams: 
    lam: float; shift: float; note: str  # armazena Œª (MLE), shift e observa√ß√µes

@dataclass
class DecompSTL:
    trend: pd.Series; seasonal: pd.Series; resid: pd.Series
    seasonal_window: int; trend_window: int; robust: bool

def inverse_boxcox(y_bc: np.ndarray, lam: float, shift: float) -> np.ndarray:
    """Invers√£o de Box‚ÄìCox para Œª=0 (log) e Œª‚â†0."""
    return (np.exp(y_bc) - shift) if np.isclose(lam, 0.0) else (np.power(lam*y_bc + 1.0, 1.0/lam) - shift)

def _make_odd(n: int) -> int: 
    """STL exige janelas √≠mpares; ajusta para o pr√≥ximo √≠mpar."""
    return int(n) if int(n) % 2 == 1 else int(n) + 1

def _auto_windows(period: int, seasonal_hint: Optional[int] = None, trend_hint: Optional[int] = None) -> Tuple[int, int]:
    """Escolha conservadora de janelas para STL com base no per√≠odo sazonal."""
    seasonal = _make_odd(max(7, (seasonal_hint or (period + 1))))
    trend = _make_odd(max(period + 1, (trend_hint or (2 * period + 1))))
    if trend <= period: trend = _make_odd(period + 1)
    return seasonal, trend

def fit_boxcox(y: pd.Series, small: float = 1e-6) -> Tuple[np.ndarray, BoxCoxParams]:
    """
    Aplica shift para positivar a s√©rie e estima Œª por MLE (scipy) -> y_bc.
    Retorna s√©rie transformada e par√¢metros (Œª, shift).
    """
    y = y.astype(float); shift = max(0.0, -float(np.nanmin(y)) + small); y_pos = y + shift
    lam = float(boxcox_normmax(y_pos.values, method="mle"))
    y_bc = boxcox(y_pos.values, lmbda=lam)
    note = (f"Box‚ÄìCox MLE Œª={lam:.3f}; shift={shift:.6g}")
    return y_bc, BoxCoxParams(lam=lam, shift=shift, note=note)

def decompose_stl(y_bc: np.ndarray, index: pd.DatetimeIndex, period: int = 12, robust: bool = True,
                  seasonal_hint: Optional[int] = None, trend_hint: Optional[int] = None) -> DecompSTL:
    """
    Decomposi√ß√£o STL em espa√ßo Box‚ÄìCox: retorna componentes e janelas usadas.
    """
    y_bc_s = pd.Series(y_bc, index=index, name="y_bc")
    seasonal_w, trend_w = _auto_windows(period, seasonal_hint, trend_hint)
    stl = STL(y_bc_s, period=period, robust=robust, seasonal=seasonal_w, trend=trend_w)
    res = stl.fit()
    return DecompSTL(trend=pd.Series(res.trend, index=index, name="trend_bc"),
                     seasonal=pd.Series(res.seasonal, index=index, name="seas_bc"),
                     resid=pd.Series(res.resid, index=index, name="resid_bc"),
                     seasonal_window=seasonal_w, trend_window=trend_w, robust=robust)

def moving_block_bootstrap(resid: np.ndarray, block_size: int, rng: np.random.Generator) -> np.ndarray:
    """
    Bootstrap em blocos m√≥veis: amostra blocos cont√≠guos dos res√≠duos para preservar depend√™ncia temporal.
    """
    n = len(resid)
    if block_size <= 1: return rng.choice(resid, size=n, replace=True)
    starts = rng.integers(0, n, size=(int(np.ceil(n / block_size)) + 1))
    pieces = []
    for st in starts:
        idx = (np.arange(st, st + block_size) % n); pieces.append(resid[idx])
        if sum(map(len, pieces)) >= n: break
    return np.concatenate(pieces)[:n]

def bootstrap_series_list(s: pd.Series, period: int = 12, n_series: int = 20, block_size: int = 24,
                          robust: bool = True, seed: int = 42) -> Tuple[List[pd.Series], Dict[str, object]]:
    """
    Gera `n_series` r√©plicas sint√©ticas seguindo FPP:
      (1) Box‚ÄìCox (Œª via MLE) + shift
      (2) STL robusta
      (3) Bootstrap dos res√≠duos (blocos)
      (4) Reconstru√ß√£o e invers√£o de Box‚ÄìCox
    Retorna lista de s√©ries e metadados √∫teis (Œª, shift, janelas).
    """
    log(f"Gerando {n_series} r√©plicas bootstrap (block={block_size})‚Ä¶")
    idx = s.index
    y_bc, bc_params = fit_boxcox(s)
    stl = decompose_stl(y_bc, idx, period=period, robust=robust)
    rng = np.random.default_rng(seed)
    out: List[pd.Series] = []
    for i in range(1, n_series+1):
        resid_boot = moving_block_bootstrap(stl.resid.values, block_size, rng)
        y_bc_star = stl.trend.values + stl.seasonal.values + resid_boot
        y_star = inverse_boxcox(y_bc_star, lam=bc_params.lam, shift=bc_params.shift)
        y_star = np.clip(y_star, 0.0, None)
        out.append(pd.Series(y_star, index=idx, name=s.name))
        if i == 1 or i % 5 == 0 or i == n_series:
            log(f"‚Ä¶ r√©plica {i}/{n_series} pronta")
    meta = {
        "n_series": n_series, "period": period, "block_size": block_size,
        "boxcox_lambda": bc_params.lam, "boxcox_shift": bc_params.shift,
        "stl_windows": {"seasonal": stl.seasonal_window, "trend": stl.trend_window},
    }
    log(f"Bootstrap conclu√≠do: {n_series} r√©plicas")
    return out, meta

# ============================
# MODELOS PARA DEMANDA INTERMITENTE (Croston/SBA/TSB)
# ============================
def _croston_core(y: np.ndarray, alpha: float = 0.1):
    """
    N√∫cleo de Croston para estimar componentes de tamanho (z) e intervalo (p).
    Retorna z, p e a s√©rie de previs√µes `f` one-step dentro da amostra.
    """
    y = np.asarray(y, dtype=float); n = len(y)
    z = np.zeros(n); p = np.zeros(n); f = np.zeros(n)
    nz = np.where(y > 0)[0]
    if len(nz) == 0: return z, p, f
    first = nz[0]; z[first] = y[first]; p[first] = 1
    f[:first+1] = z[first] / max(p[first], 1e-9)
    q = 0
    for t in range(first+1, n):
        if y[t] > 0:
            q += 1
            z[t] = z[t-1] + alpha * (y[t] - z[t-1])
            p[t] = p[t-1] + alpha * (q - p[t-1]); q = 0
        else:
            z[t] = z[t-1]; p[t] = p[t-1]; q += 1
        f[t] = z[t] / max(p[t], 1e-9)
    return z, p, f

def croston_forecast(y: np.ndarray, alpha: float, h: int):
    """Croston puro: previs√£o constante para horizonte h baseada no √∫ltimo f."""
    _, _, f = _croston_core(y, alpha); last_f = f[-1] if len(f) else 0.0
    return f, np.full(h, last_f)

def sba_forecast(y: np.ndarray, alpha: float, h: int):
    """SBA: corre√ß√£o de vi√©s (1 - alpha/2) sobre Croston."""
    _, _, f = _croston_core(y, alpha); f_adj = f * (1 - alpha/2.0)
    last_f = f_adj[-1] if len(f_adj) else 0.0
    return f_adj, np.full(h, last_f)

def tsb_forecast(y: np.ndarray, alpha: float, beta: float, h: int):
    """
    TSB: suaviza separadamente a probabilidade de ocorr√™ncia (p) e o tamanho (z).
    √ötil quando zeros e positivos se alternam.
    """
    y = np.asarray(y, dtype=float); n = len(y); p = np.zeros(n); z = np.zeros(n); f = np.zeros(n)
    p[0] = 1.0 if np.any(y>0) else 0.0; z[0] = y[y>0].mean() if np.any(y>0) else 0.0; f[0] = p[0]*z[0]
    for t in range(1, n):
        occ = 1.0 if y[t]>0 else 0.0
        p[t] = p[t-1] + beta*(occ - p[t-1])
        z[t] = z[t-1] + alpha*(((y[t] if occ==1.0 else z[t-1]) - z[t-1]))
        f[t] = p[t]*z[t]
    last_f = f[-1] if len(f) else 0.0
    return f, np.full(h, last_f)

# ============================
# SUPERVIS√ÉO PARA RANDOM FOREST
# ============================
def make_supervised_from_series(s: pd.Series, lags: list) -> pd.DataFrame:
    """
    Constr√≥i DataFrame com alvo 'y', lags 1..k e dummies de m√™s.
    Usado para RandomForest.
    """
    df = pd.DataFrame({"y": s.values}, index=s.index)
    for L in lags: df[f"lag_{L}"] = df["y"].shift(L)
    df["month"] = df.index.month
    df = pd.get_dummies(df, columns=["month"], drop_first=True).dropna()
    return df

# ============================
# LSTM (opcional)
# ============================
def _make_sequences(arr, window):
    """Cria janelas deslizantes (X,y) para s√©ries normalizadas."""
    X, y = [], []
    for i in range(window, len(arr)): X.append(arr[i-window:i]); y.append(arr[i])
    return np.array(X), np.array(y)

def lstm_fit_predict(s: pd.Series, horizon: int, window: int, epochs: int, batch: int):
    """
    Treina LSTM minimalista para previs√£o one-shot do bloco de teste (√∫ltimo `horizon`).
    Retorna (y_test_invertida, y_pred_invertida, None, runtime).
    """
    if not KERAS_AVAILABLE: raise RuntimeError("TensorFlow/Keras n√£o dispon√≠vel.")
    values = s.values.reshape(-1,1)
    scaler = MinMaxScaler(); scaled = scaler.fit_transform(values)
    X, y = _make_sequences(scaled, window)
    if len(X) <= horizon: raise RuntimeError("S√©rie insuficiente para LSTM no split.")
    X_train, X_test = X[:-horizon], X[-horizon:]; y_train, y_test = y[:-horizon], y[-horizon:]
    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
    X_test  = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))
    model = Sequential([LSTM(64, input_shape=(window,1)), Dense(1)])
    model.compile(optimizer="adam", loss="mse")
    t0 = time.time(); model.fit(X_train, y_train, epochs=epochs, batch_size=batch, verbose=0)
    y_pred_test_scaled = model.predict(X_test, verbose=0); runtime = time.time() - t0
    y_test_inv = scaler.inverse_transform(y_test.reshape(-1,1)).ravel()
    y_pred_inv = scaler.inverse_transform(y_pred_test_scaled).ravel()
    return y_test_inv, y_pred_inv, None, runtime

# ============================
# AVALIA√á√ÉO DE MODELOS EM UMA S√âRIE (para um dado pr√©-processamento)
# ============================
@dataclass
class ModelResult:
    """Linha de resultado que compor√° a tabela final."""
    preprocess: str; preprocess_params: str; model: str; model_params: str
    mae: float; mape: float; rmse: float; smape_: float
    train_size: int; test_size: int; runtime_s: float

def evaluate_models_on_series(
    base_series: pd.Series, horizon: int, seasonal_period: int,
    preprocess_label: str, preprocess_params: str,
    forward_transform: Optional[Callable[[pd.Series], pd.Series]] = None,
    inverse_transform: Optional[Callable[[np.ndarray], np.ndarray]] = None
) -> List[ModelResult]:
    """
    Executa todos os modelos sobre `base_series`, j√° aplicando (ou n√£o) uma transforma√ß√£o.
    Se `inverse_transform` √© fornecida, as m√©tricas s√£o calculadas na escala original (recomendado).
    Retorna lista de ModelResult (uma linha por combina√ß√£o de hiperpar√¢metros/modelo).
    """

    with Timer(f"Testes ‚Äî {preprocess_label} ({preprocess_params})"):
        # 1) aplica transforma√ß√£o (se houver) e faz sanitiza√ß√£o b√°sica
        s_model = forward_transform(base_series) if forward_transform else base_series
        s_model = pd.Series(s_model.values, index=base_series.index, dtype=float)
        s_model = s_model.replace([np.inf, -np.inf], np.nan).interpolate("linear").bfill().ffill()
        if len(s_model.dropna()) < horizon + 2:
            raise ValueError("S√©rie muito curta ap√≥s preparo. Garanta pelo menos horizon+2 observa√ß√µes.")

        results: List[ModelResult] = []
        hist_all = s_model.iloc[:-horizon].values    # janela de treino
        test_vals = s_model.iloc[-horizon:].values   # janela de teste (holdout)

        def _metrics(y_true_mdl, y_pred_mdl):
            """
            Converte (se preciso) para a escala original antes de computar as m√©tricas.
            """
            if inverse_transform:
                y_true = inverse_transform(y_true_mdl); y_pred = inverse_transform(y_pred_mdl)
            else:
                y_true, y_pred = y_true_mdl, y_pred_mdl
            return eval_metrics(y_true, y_pred)

        # ---- CROSTON
        log(f"‚Üí Croston: {len(CROSTON_ALPHAS)} alphas")
        for j, alpha in enumerate(CROSTON_ALPHAS, 1):
            t0 = time.time()
            # walk-forward simples: a cada passo, prev√™ 1 e anexa o valor real
            hist = hist_all.copy(); preds = []
            for i in range(horizon):
                _, f1 = croston_forecast(hist, alpha, 1)
                preds.append(f1[0]); hist = np.append(hist, test_vals[i])
            y_pred = np.array(preds, dtype=float)
            if not np.all(np.isfinite(y_pred)):
                log(f"[WARN] {preprocess_label} | Croston alpha={alpha} -> y_pred inv√°lido; pulando.")
                continue
            mets = _metrics(test_vals, y_pred); runtime = time.time() - t0
            results.append(ModelResult(preprocess_label, preprocess_params, "Croston", f"alpha={alpha}",
                                       mets["MAE"], mets["MAPE"], mets["RMSE"], mets["sMAPE"],
                                       len(s_model)-horizon, horizon, runtime))
            if j == 1 or j == len(CROSTON_ALPHAS):
                log(f"‚Ä¶ Croston progresso {j}/{len(CROSTON_ALPHAS)}")

        # ---- SBA
        log(f"‚Üí SBA: {len(SBA_ALPHAS)} alphas")
        for j, alpha in enumerate(SBA_ALPHAS, 1):
            t0 = time.time()
            hist = hist_all.copy(); preds = []
            for i in range(horizon):
                _, f1 = sba_forecast(hist, alpha, 1)
                preds.append(f1[0]); hist = np.append(hist, test_vals[i])
            y_pred = np.array(preds, dtype=float)
            if not np.all(np.isfinite(y_pred)):
                log(f"[WARN] {preprocess_label} | SBA alpha={alpha} -> y_pred inv√°lido; pulando.")
                continue
            mets = _metrics(test_vals, y_pred); runtime = time.time() - t0
            results.append(ModelResult(preprocess_label, preprocess_params, "SBA", f"alpha={alpha}",
                                       mets["MAE"], mets["MAPE"], mets["RMSE"], mets["sMAPE"],
                                       len(s_model)-horizon, horizon, runtime))
            if j == 1 or j == len(SBA_ALPHAS):
                log(f"‚Ä¶ SBA progresso {j}/{len(SBA_ALPHAS)}")

        # ---- TSB
        tot_tsb = len(TSB_ALPHA_GRID) * len(TSB_BETA_GRID)
        log(f"‚Üí TSB: {tot_tsb} combina√ß√µes (alpha x beta)")
        step = 0
        for alpha in TSB_ALPHA_GRID:
            for beta in TSB_BETA_GRID:
                step += 1
                t0 = time.time()
                hist = hist_all.copy(); preds = []
                for i in range(horizon):
                    _, f1 = tsb_forecast(hist, alpha, beta, 1)
                    preds.append(f1[0]); hist = np.append(hist, test_vals[i])
                y_pred = np.array(preds, dtype=float)
                if not np.all(np.isfinite(y_pred)):
                    log(f"[WARN] {preprocess_label} | TSB alpha={alpha}, beta={beta} -> y_pred inv√°lido; pulando.")
                    continue
                mets = _metrics(test_vals, y_pred); runtime = time.time() - t0
                results.append(ModelResult(preprocess_label, preprocess_params, "TSB",
                                           f"alpha={alpha}, beta={beta}",
                                           mets["MAE"], mets["MAPE"], mets["RMSE"], mets["sMAPE"],
                                           len(s_model)-horizon, horizon, runtime))
                if step == 1 or step == tot_tsb:
                    log(f"‚Ä¶ TSB progresso {step}/{tot_tsb}")

        # ---- RandomForest
        tot_rf = len(RF_LAGS_GRID) * len(RF_N_ESTIMATORS_GRID) * len(RF_MAX_DEPTH_GRID)
        log(f"‚Üí RandomForest: {tot_rf} combina√ß√µes (lags x n_estimators x max_depth)")
        cnt = 0
        for k in RF_LAGS_GRID:
            for n_est in RF_N_ESTIMATORS_GRID:
                for max_depth in RF_MAX_DEPTH_GRID:
                    cnt += 1
                    lags = list(range(1, k+1))
                    df_sup = make_supervised_from_series(s_model, lags)
                    if len(df_sup) <= horizon:
                        log(f"[WARN] {preprocess_label} | RF lags=1..{k} -> dados insuficientes; pulando.")
                        continue
                    y = df_sup["y"].values
                    X = df_sup.drop(columns=["y"]).values
                    # split simples: √∫ltimas `horizon` linhas para teste
                    X_train, X_test = X[:-horizon], X[-horizon:]
                    y_train, y_test = y[:-horizon], y[-horizon:]
                    t0 = time.time()
                    model = RandomForestRegressor(n_estimators=n_est, random_state=RANDOM_STATE, max_depth=max_depth)
                    model.fit(X_train, y_train)
                    y_pred = model.predict(X_test).astype(float); runtime = time.time() - t0
                    if not np.all(np.isfinite(y_pred)):
                        log(f"[WARN] {preprocess_label} | RF lags=1..{k} -> y_pred inv√°lido; pulando.")
                        continue
                    mets = _metrics(y_test, y_pred)
                    results.append(ModelResult(preprocess_label, preprocess_params, "RandomForest",
                                               f"lags=1..{k}, n_estimators={n_est}, max_depth={max_depth}",
                                               mets["MAE"], mets["MAPE"], mets["RMSE"], mets["sMAPE"],
                                               len(s_model)-horizon, horizon, runtime))
                    if cnt == 1 or cnt == tot_rf:
                        log(f"‚Ä¶ RF progresso {cnt}/{tot_rf}")

        # ---- SARIMAX
        combos = list(itertools.product(SARIMA_GRID["p"], SARIMA_GRID["d"], SARIMA_GRID["q"],
                                        SARIMA_GRID["P"], SARIMA_GRID["D"], SARIMA_GRID["Q"]))
        log(f"‚Üí SARIMAX: {len(combos)} combina√ß√µes")
        y_train = s_model.iloc[:-horizon]; y_test  = s_model.iloc[-horizon:]
        for i, (p,d,q,P,D,Q) in enumerate(combos, 1):
            try:
                t0 = time.time()
                model = SARIMAX(y_train, order=(p,d,q), seasonal_order=(P,D,Q,seasonal_period),
                                enforce_stationarity=False, enforce_invertibility=False)
                res = model.fit(disp=False)
                y_pred = res.get_forecast(steps=horizon).predicted_mean.values.astype(float)
                runtime = time.time() - t0
                if not np.all(np.isfinite(y_pred)):
                    log(f"[WARN] {preprocess_label} | SARIMAX({p},{d},{q})x({P},{D},{Q},{seasonal_period}) -> y_pred inv√°lido; pulando.")
                    continue
                mets = _metrics(y_test.values, y_pred)
                params = f"order=({p},{d},{q}), seasonal=({P},{D},{Q},{seasonal_period}), AIC={res.aic:.2f}"
                results.append(ModelResult(preprocess_label, preprocess_params, "SARIMAX",
                                           params, mets["MAE"], mets["MAPE"], mets["RMSE"], mets["sMAPE"],
                                           len(s_model)-horizon, horizon, runtime))
                if i == 1 or i % 10 == 0 or i == len(combos):
                    log(f"‚Ä¶ SARIMAX progresso {i}/{len(combos)}")
            except Exception as e:
                # falhas comuns: n√£o converg√™ncia; par√¢metros n√£o invert√≠veis etc.
                log(f"[WARN] {preprocess_label} | SARIMAX({p},{d},{q})x({P},{D},{Q},{seasonal_period}) -> erro: {e}")

        # ---- LSTM (se dispon√≠vel)
        if KERAS_AVAILABLE:
            lstm_combos = [(6,30,16),(12,30,16)]
            log(f"‚Üí LSTM: {len(lstm_combos)} combina√ß√µes")
            for c_idx, (window,epochs,batch) in enumerate(lstm_combos, 1):
                try:
                    y_test_inv, y_pred_inv, _, runtime = lstm_fit_predict(s_model, horizon, window, epochs, batch)
                    if inverse_transform:
                        # quando a s√©rie foi transformada (ex.: log), faz invers√£o antes de medir
                        y_true = inverse_transform(s_model.iloc[-horizon:].values)
                        y_pred = inverse_transform(y_pred_inv)
                        mets = eval_metrics(y_true, y_pred)
                    else:
                        mets = eval_metrics(y_test_inv, y_pred_inv)
                    params = f"window={window}, epochs={epochs}, batch={batch}, units=64"
                    results.append(ModelResult(preprocess_label, preprocess_params, "LSTM",
                                               params, mets["MAE"], mets["MAPE"], mets["RMSE"], mets["sMAPE"],
                                               len(s_model)-horizon, horizon, runtime))
                    if c_idx == 1 or c_idx == len(lstm_combos):
                        log(f"‚Ä¶ LSTM progresso {c_idx}/{len(lstm_combos)}")
                except Exception as e:
                    log(f"[WARN] {preprocess_label} | LSTM window={window} -> erro: {e}")
        else:
            log("[LSTM] TensorFlow n√£o encontrado; pulando LSTM.")

        log(f"‚úì Conclu√≠dos testes: {preprocess_label} ({len(results)} linhas)")
        return results

# ============================
# RANKEAMENTO E SELE√á√ÉO DO CAMPE√ÉO
# ============================
def _simplicity_rank(model_name: str) -> int:
    """
    Ordem de simplicidade (para desempate final). Menor √© melhor.
    üîå ajuste livre se quiser priorizar m√©todos internos da empresa.
    """
    order = ["NaiveSeasonal","Croston","SBA","TSB","SARIMAX","RandomForest","LSTM"]
    try: return order.index(model_name)
    except ValueError: return len(order)

def select_champion(df: pd.DataFrame) -> pd.Series:
    """
    Crit√©rio principal: menor MAE (escala original).
    Desempates: menor RMSE -> menor (MAE+RMSE) -> modelo mais simples.
    Retorna a linha campe√£ como Series.
    """
    best = df[df["MAE"] == df["MAE"].min()].copy()
    best = best[best["RMSE"] == best["RMSE"].min()]
    if len(best) > 1:
        best["MAE_RMSE_SUM"] = best["MAE"] + best["RMSE"]
        best = best[best["MAE_RMSE_SUM"] == best["MAE_RMSE_SUM"].min()]
    if len(best) > 1:
        best["_simp"] = best["model"].apply(_simplicity_rank)
        best = best[best["_simp"] == best["_simp"].min()]
    return best.iloc[0]

# ============================
# ORQUESTRADOR GERAL
# ============================
def run_full_pipeline(
    data_input: Union[str, pd.DataFrame, pd.Series],
    sheet_name: Optional[str] = None, date_col: Optional[str] = None, value_col: Optional[str] = None,
    horizon: int = 6, seasonal_period: int = 12,
    do_original: bool = True, do_log: bool = True, do_bootstrap: bool = True,
    n_bootstrap: int = 20, bootstrap_block: int = 24,
    save_dir: Optional[str] = None
) -> pd.DataFrame:
    """
    Roda o pipeline completo sobre uma √∫nica s√©rie:
    - compara 3 pr√©-processamentos (original / log+Œµ / bootstrap FPP)
    - treina/avalia todos os modelos
    - compila a tabela e escolhe o campe√£o

    üîå Streamlit hint:
    - Exponha `horizon`, `seasonal_period`, flags `do_*`, `n_bootstrap` e `bootstrap_block` como widgets.
    - Conecte `save_dir` a um diret√≥rio tempor√°rio e ofere√ßa bot√µes de download.
    """
    log("==== PIPELINE INICIADO ====")
    log(f"Params: horizon={horizon}, season={seasonal_period}, original={do_original}, log={do_log}, bootstrap={do_bootstrap}")
    if do_bootstrap: log(f"Bootstrap: n_replicas={n_bootstrap}, block={bootstrap_block}")
    log(f"LSTM dispon√≠vel: {KERAS_AVAILABLE}")

    base_series = load_series(data_input, sheet_name=sheet_name, date_col=date_col, value_col=value_col)

    all_results: List[ModelResult] = []

    # ORIGINAL
    if do_original:
        log("Realizando testes da s√©rie ORIGINAL‚Ä¶")
        all_results += evaluate_models_on_series(
            base_series=base_series, horizon=horizon, seasonal_period=seasonal_period,
            preprocess_label="original", preprocess_params="-",
            forward_transform=None, inverse_transform=None
        )

    # LOG + Œµ
    if do_log:
        log("Preparando transforma√ß√£o LOG‚Ä¶")
        fwd, inv, params_txt = make_log_transformers(base_series, window=6)
        log("Realizando testes da s√©rie LOG-transformada‚Ä¶")
        all_results += evaluate_models_on_series(
            base_series=base_series, horizon=horizon, seasonal_period=seasonal_period,
            preprocess_label="log", preprocess_params=params_txt,
            forward_transform=fwd, inverse_transform=inv
        )

    # BOOTSTRAP
    if do_bootstrap:
        with Timer("Gera√ß√£o das r√©plicas sint√©ticas (bootstrap)"):
            series_list, meta = bootstrap_series_list(
                base_series, period=seasonal_period, n_series=n_bootstrap,
                block_size=bootstrap_block, robust=True, seed=RANDOM_STATE
            )
        log(f"{meta['n_series']} r√©plicas geradas (Œª={meta['boxcox_lambda']:.3f}, shift={meta['boxcox_shift']:.6g})")

        for i, s_rep in enumerate(series_list, start=1):
            log(f"Realizando testes da S√âRIE SINT√âTICA {i}/{len(series_list)}‚Ä¶")
            params_txt = (f"replica={i}, block_size={bootstrap_block}, "
                          f"lambda={meta['boxcox_lambda']:.3f}, shift={meta['boxcox_shift']:.6g}")
            all_results += evaluate_models_on_series(
                base_series=s_rep, horizon=horizon, seasonal_period=seasonal_period,
                preprocess_label="bootstrap", preprocess_params=params_txt,
                forward_transform=None, inverse_transform=None
            )

    # Consolida a lista de ModelResult em DataFrame de experimentos
    rows = [{
        "preprocess": r.preprocess, "preprocess_params": r.preprocess_params,
        "model": r.model, "model_params": r.model_params,
        "MAE": r.mae, "MAPE": r.mape, "RMSE": r.rmse, "sMAPE": r.smape_,
        "Train Size": r.train_size, "Test Size": r.test_size, "Runtime (s)": r.runtime_s,
    } for r in all_results]

    df_out = pd.DataFrame(rows)

    # Sele√ß√£o do CAMPE√ÉO segundo FPP3 (menor MAE; desempates RMSE, soma, simplicidade)
    champion = select_champion(df_out)
    log("===== CAMPE√ÉO (crit√©rio: menor MAE; desempates por RMSE/soma/simplicidade) =====")
    log(f"Preprocess: {champion['preprocess']} | Params: {champion['preprocess_params']}")
    log(f"Modelo: {champion['model']} | Hiperpar√¢metros: {champion['model_params']}")
    log(f"MAE={champion['MAE']:.6g} | RMSE={champion['RMSE']:.6g} | MAPE={champion['MAPE']:.6g} | sMAPE={champion['sMAPE']:.6g}")

    # Ordena√ß√£o leve para visualiza√ß√£o (n√£o afeta o campe√£o j√° escolhido)
    df_out = df_out.sort_values(by=["preprocess","model","MAE","RMSE"]).reset_index(drop=True)

    # === (NOVO) Forecast real do campe√£o (refit na s√©rie completa) ===
    try:
        forecast_df = _refit_and_forecast_champion(
            base_series=base_series,
            champion_row=champion,
            horizon=horizon,
            seasonal_period=seasonal_period
        )
        log(f"[OK] Previs√£o futura gerada com o campe√£o: {len(forecast_df)} meses")
    except Exception as e:
        log(f"[WARN] Falha ao gerar previs√£o futura do campe√£o: {e}")
        # fallback vazio ‚Äî a p√°gina decidir√° o que fazer
        forecast_df = pd.DataFrame({"ds": [], "y": []})

    # Persist√™ncia dos resultados
    if save_dir:
        os.makedirs(save_dir, exist_ok=True)
        xlsx_path = os.path.join(save_dir, "experimentos_unificado.xlsx")
        csv_path  = os.path.join(save_dir, "experimentos_unificado.csv")
        champion_path = os.path.join(save_dir, "champion.csv")
        forecast_path = os.path.join(save_dir, "forecast_campeao.csv")
        with pd.ExcelWriter(xlsx_path, engine="openpyxl") as writer:
            df_out.to_excel(writer, sheet_name="experiments", index=False)
            pd.DataFrame([champion]).to_excel(writer, sheet_name="champion", index=False)
            if len(forecast_df):
                forecast_df.to_excel(writer, sheet_name="forecast", index=False)
        df_out.to_csv(csv_path, index=False)
        pd.DataFrame([champion]).to_csv(champion_path, index=False)
        if len(forecast_df):
            forecast_df.to_csv(forecast_path, index=False)
        log(f"[OK] Resultados salvos em:\n - {xlsx_path}\n - {csv_path}\n - {champion_path}\n - {forecast_path if len(forecast_df) else '(sem forecast)'}")

    log(f"==== PIPELINE FINALIZADO ====\nLinhas totais de experimentos: {len(df_out)}")
    log("Resumo r√°pido por preprocess:")
    resumo = df_out.groupby("preprocess").size().to_dict()
    for k, v in resumo.items():
        log(f"  ‚Ä¢ {k}: {v} linhas")

    # Guarda o campe√£o e a PREVIS√ÉO como atributos do DataFrame para o app
    df_out.attrs["champion"] = champion.to_dict()
    df_out.attrs["forecast_df"] = forecast_df.copy()
    return df_out


# === (ADICIONE) UTIL: r√≥tulos futuros mensais =========================
def _future_month_labels(last_ts: pd.Timestamp, h: int) -> list[str]:
    _PT = {1:"Jan",2:"Fev",3:"Mar",4:"Abr",5:"Mai",6:"Jun",7:"Jul",8:"Ago",9:"Set",10:"Out",11:"Nov",12:"Dez"}
    fut_idx = pd.date_range(last_ts + pd.offsets.MonthBegin(1), periods=h, freq="MS")
    return [f"{_PT[d.month]}/{str(d.year)[-2:]}" for d in fut_idx]

# === (ADICIONE) UTIL: transformar conforme preprocess do campe√£o =======
def _get_transforms_for_preprocess(preprocess_label: str, base_series: pd.Series):
    """
    Retorna (forward_transform, inverse_transform, params_txt) para o r√≥tulo do campe√£o.
    - 'original' -> (None, None, "-")
    - 'log'      -> (fwd, inv, "epsilon=..., shift=..., score=...")
    - 'bootstrap' -> por defini√ß√£o s√£o s√©ries sint√©ticas; para previs√£o final usamos a s√©rie original
                     (isto √©, tratamos como 'original' aqui).
    """
    if preprocess_label == "log":
        fwd, inv, params_txt = make_log_transformers(base_series, window=6)
        return fwd, inv, params_txt
    # fallback: original
    return None, None, "-"

# === (ADICIONE) REFIT + FORECAST DO CAMPE√ÉO ============================
def _parse_params(text: str) -> dict:
    """
    Extrai pares simples de 'chave=valor' de strings como:
      - "alpha=0.2"
      - "alpha=0.3, beta=0.5"
      - "order=(1,1,1), seasonal=(0,1,1,12), AIC=123.45"
      - "lags=1..12, n_estimators=200, max_depth=None"
    Retorna dict com valores num√©ricos/tuplas quando poss√≠vel.
    """
    out = {}
    if not isinstance(text, str):
        return out
    try:
        parts = [p.strip() for p in text.split(",")]
        for p in parts:
            if "=" not in p: 
                continue
            k, v = [x.strip() for x in p.split("=", 1)]
            # tuplas
            if v.startswith("(") and v.endswith(")"):
                try:
                    out[k] = eval(v)
                    continue
                except Exception:
                    pass
            # intervalos estilo "1..12"
            if ".." in v:
                a, b = v.split("..")
                out[k] = (int(a), int(b))
                continue
            # None
            if v.lower() == "none":
                out[k] = None
                continue
            # num√©rico
            try:
                if "." in v or "e" in v.lower():
                    out[k] = float(v)
                else:
                    out[k] = int(v)
                continue
            except Exception:
                pass
            out[k] = v
    except Exception:
        pass
    return out

def _refit_and_forecast_champion(base_series: pd.Series,
                                 champion_row: pd.Series,
                                 horizon: int,
                                 seasonal_period: int) -> pd.DataFrame:
    """
    Reajusta o modelo campe√£o na S√âRIE COMPLETA (com a mesma transforma√ß√£o do preprocess)
    e gera previs√£o h passos √† frente em escala ORIGINAL.
    Retorna DataFrame {"ds": labels_m√™s_pt, "y": valores >= 0}.
    """
    model = str(champion_row["model"])
    params = str(champion_row["model_params"])
    preprocess = str(champion_row["preprocess"])

    # 1) transforma√ß√£o conforme preprocess campe√£o
    fwd, inv, _ = _get_transforms_for_preprocess(preprocess, base_series)
    s_fit = fwd(base_series) if fwd else base_series
    s_fit = pd.Series(s_fit.values, index=base_series.index, dtype=float)
    s_fit = s_fit.replace([np.inf, -np.inf], np.nan).interpolate("linear").bfill().ffill()

    # 2) previs√£o por fam√≠lia
    y_pred_model = None
    P = _parse_params(params)

    if model == "SARIMAX":
        order = P.get("order", (0,1,1))
        seasonal = P.get("seasonal", (0,1,1, seasonal_period))
        res = SARIMAX(s_fit, order=tuple(order), seasonal_order=tuple(seasonal),
                      enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)
        y_pred_model = res.get_forecast(steps=horizon).predicted_mean.values.astype(float)

    elif model in ("Croston", "SBA", "TSB"):
        hist = s_fit.values.astype(float)
        if model == "Croston":
            alpha = float(P.get("alpha", 0.1))
            _, y_pred_model = croston_forecast(hist, alpha, horizon)
        elif model == "SBA":
            alpha = float(P.get("alpha", 0.1))
            _, y_pred_model = sba_forecast(hist, alpha, horizon)
        else:  # TSB
            alpha = float(P.get("alpha", 0.3)); beta = float(P.get("beta", 0.3))
            _, y_pred_model = tsb_forecast(hist, alpha, beta, horizon)

    elif model == "RandomForest":
        # Precisamos gerar features recursivamente para os pr√≥ximos h passos
        lpair = P.get("lags", P.get("lags=1..k", (1, 12)))
        if isinstance(lpair, tuple):
            max_lag = max(lpair)
        else:
            # se veio "lags=1..12" transformado em (1,12)
            max_lag = int(lpair) if isinstance(lpair, (int, float)) else 12
        n_est = int(P.get("n_estimators", 200))
        max_depth = P.get("max_depth", None)

        # Treina no passado
        df_sup = make_supervised_from_series(s_fit, list(range(1, max_lag+1)))
        y = df_sup["y"].values
        X = df_sup.drop(columns=["y"]).values
        rf = RandomForestRegressor(n_estimators=n_est, random_state=RANDOM_STATE, max_depth=max_depth)
        rf.fit(X, y)

        # Prev√™ recursivamente h meses
        cur = s_fit.copy()
        preds = []
        last_ts = cur.index[-1]
        for _ in range(horizon):
            # monta features do pr√≥ximo m√™s
            tmp = pd.DataFrame({"y": cur.values}, index=cur.index)
            for L in range(1, max_lag+1):
                tmp[f"lag_{L}"] = tmp["y"].shift(L)
            # linha "do pr√≥ximo m√™s": use os √∫ltimos valores conhecidos
            row = tmp.iloc[-1:].copy()
            # como vamos prever t+1, nossos lags para t+1 s√£o exatamente tmp.iloc[-1, lag_*]
            feat = row.drop(columns=["y"]).values
            # completa dummies de m√™s como em make_supervised_from_series
            # (reconstroi rapidinho com o mesmo esquema)
            next_month = (last_ts + pd.offsets.MonthBegin(1)).month
            base_cols = [c for c in df_sup.drop(columns=["y"]).columns if not c.startswith("month_")]
            month_cols = [c for c in df_sup.drop(columns=["y"]).columns if c.startswith("month_")]
            x_vec = np.zeros((1, len(base_cols)+len(month_cols)), dtype=float)

            # preencher lags (na mesma ordem de df_sup)
            # reconstr√≥i ordem exata:
            X_cols = list(df_sup.drop(columns=["y"]).columns)
            x_map = {}
            for i, c in enumerate(base_cols):
                # localiza valor do feat conforme coluna
                try:
                    idx = list(row.drop(columns=["y"]).columns).index(c)
                    x_map[c] = float(feat[0, idx])
                except Exception:
                    x_map[c] = np.nan
            for mc in month_cols:
                # month_2..month_12 (drop_first=True no treinamento)
                want = int(mc.split("_")[1])
                x_map[mc] = 1.0 if want == next_month else 0.0

            # reordena conforme X_cols
            X_next = np.array([[x_map.get(c, 0.0) for c in X_cols]], dtype=float)
            y_next = float(rf.predict(X_next)[0])
            preds.append(y_next)
            # anexa no fim e anda o tempo
            last_ts = last_ts + pd.offsets.MonthBegin(1)
            cur = pd.concat([cur, pd.Series([y_next], index=[last_ts])])

        y_pred_model = np.array(preds, dtype=float)

    elif model == "LSTM":
        # Para manter simples, refaz o treinamento e previs√£o como no holdout,
        # por√©m expandindo para 'h' passos. Se TensorFlow n√£o estiver dispon√≠vel,
        # protegemos com mensagem clara (mas no seu pipeline j√° tratamos KERAS_AVAILABLE).
        if not KERAS_AVAILABLE:
            raise RuntimeError("TensorFlow/Keras indispon√≠vel para refit do LSTM.")
        # Reaproveita a fun√ß√£o j√° existente:
        # Estrat√©gia: usamos janela 'window' padr√£o = 12 (ou o que voc√™ preferir).
        window, epochs, batch = 12, 30, 16
        # Treinamos sobre toda a s√©rie e usamos abordagem recursiva semelhante ao RF,
        # mas mantendo a normaliza√ß√£o; para brevidade, chamamos a fun√ß√£o de treino
        # e depois iteramos h vezes com a janela mais recente.
        # (Implementa√ß√£o enxuta ‚Äî suficiente p/ produ√ß√£o leve)
        from sklearn.preprocessing import MinMaxScaler
        values = s_fit.values.reshape(-1,1)
        scaler = MinMaxScaler(); scaled = scaler.fit_transform(values)
        def _make_seq(arr, w):
            X, y = [], []
            for i in range(w, len(arr)):
                X.append(arr[i-w:i])
                y.append(arr[i])
            return np.array(X), np.array(y)
        X, y = _make_seq(scaled, window)
        X = X.reshape((X.shape[0], X.shape[1], 1))
        model_ = Sequential([LSTM(64, input_shape=(window,1)), Dense(1)])
        model_.compile(optimizer="adam", loss="mse")
        model_.fit(X, y, epochs=epochs, batch_size=batch, verbose=0)

        # recursivo h passos √† frente
        seq = scaled[-window:].copy()
        preds_scaled = []
        for _ in range(horizon):
            x_in = seq.reshape((1, window, 1))
            y_hat = model_.predict(x_in, verbose=0)[0,0]
            preds_scaled.append(y_hat)
            seq = np.vstack([seq[1:], [y_hat]])
        y_pred_inv = scaler.inverse_transform(np.array(preds_scaled).reshape(-1,1)).ravel()
        y_pred_model = y_pred_inv.astype(float)

    else:
        # prote√ß√£o: caso apare√ßa modelo inesperado
        raise RuntimeError(f"Modelo campe√£o n√£o suportado para refit: {model}")

    # 3) invers√£o para escala ORIGINAL (se necess√°rio)
    if inv:
        y_future = inv(y_pred_model)
    else:
        y_future = y_pred_model

    # saneamento final (sem negativos)
    y_future = np.clip(np.asarray(y_future, dtype=float), 0.0, None)

    # 4) DataFrame final com r√≥tulos "M√™s/AA"
    last_ts = base_series.index[-1]
    labels = _future_month_labels(last_ts, horizon)
    return pd.DataFrame({"ds": labels, "y": y_future})

# ============================
# EXECU√á√ÉO LOCAL (EXEMPLO)
# ============================
if __name__ == "__main__":
    # üîå Streamlit hint:
    # - No app, essas strings viram par√¢metros (input file uploader e pasta de sa√≠da).
    CAMINHO = "/Users/felipe/Desktop/Faculdade/TCC/CoÃÅdigos VS Code/Streamlit/(2017-2025) SeÃÅrie Temporal - Prod Cod 7 (A).xlsx"
    SAIDA   = "/Users/felipe/Desktop/Faculdade/TCC/CoÃÅdigos VS Code/Streamlit"

    with Timer("Rodando pipeline completo"):
        resultados = run_full_pipeline(
            data_input=CAMINHO,
            sheet_name=None, date_col=None, value_col=None,
            horizon=6, seasonal_period=12,
            do_original=True, do_log=True, do_bootstrap=True,
            n_bootstrap=20,         # ajuste livre
            bootstrap_block=24,     # refer√™ncia p/ mensal
            save_dir=SAIDA
        )

    # Pr√©-visualiza√ß√£o e log do campe√£o
    log("Pr√©via do resultado (top 10 linhas):")
    print(resultados.head(10).to_string(index=False))
    champ = resultados.attrs.get("champion", {})
    if champ:
        log("RESUMO CAMPE√ÉO:")
        for k, v in champ.items():
            log(f"  {k}: {v}")
